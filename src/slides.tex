\documentclass{beamer}

\usetheme{Padova}

\title{Condorcet Fusion: an implementation}
\subtitle{Improved Retrieval through Rank Fusion}
\author{Marco Alfonso, Davide Martini, Giovanni Mazzocchin}
\date{February 9, 2018}


\begin{document}

	\maketitle

	%\begin{frame}{Outline}
	%	\tableofcontents
	%\end{frame}


	\section{Introduction}

	\begin{frame}{Introduction}

		This project implements some ideas coming from a paper by 
                \newline \textbf {Aslam} and \textbf{Montague}. 
                \newline
                \newline
		These authors applied \textit{Social Choice Theory} to \textbf{Rank Fusion}. %\vspace{.5em}
                \newline
                \newline
                They claim that their algorithm beats the ones by \textbf {Fox} and \textbf{Shaw}
                most of the time.
		%\begin{itemize}
		%	\item Morbi \textbf{vitae lacus} porta neque tincidunt sodales \vspace{.5em}
		%	\item Proin tincidunt, \textbf{neque} at tincidunt mollis \vspace{.5em}
		%	\item Ut \alert{lacinia sem a nibh} consequat porttitor
		%\end{itemize}
	\end{frame}


	\section{Starter: basic fusion strategies}

	\begin{frame}{Starter: basic fusion strategies}
                \textbf {Fox} and \textbf{Shaw} devised some simple, score-based fusion formulae:
		\begin{block}{CombMAX}
		       \textit{fused score = max(scores)} 
		\end{block}

		\begin{alertblock}{CombMIN}
		       \textit{fused score = min(scores)}   
		\end{alertblock}

		\begin{exampleblock}{CombSUM}
		       \textit{fused score = sum(scores)}  
		\end{exampleblock}
	\end{frame}

        \section{Starter: basic fusion strategies}

	\begin{frame}{Starter: basic fusion strategies}
                \textbf {Fox} and \textbf{Shaw} devised some simple, score-based fusion formulae:
		\begin{block}{CombMED}
		       \textit{fused score = median(scores)} 
		\end{block}

		\begin{alertblock}{CombANZ}
		       \textit{fused score = $\frac{CombSUM}{\#nonZeroScores}$}   
		\end{alertblock}

		\begin{exampleblock}{CombMNZ}
		       \textit{fused score = $CombSUM \times \#nonZeroScores$}  
		\end{exampleblock}
	\end{frame}

        \section{Are we comparing apples and oranges}

        \begin{frame}{Are we comparing apples and oranges?}
               Of course we can't perform any sensible fusion without \textbf{normalization}. \newline
               \textbf{Lee} came up with two handy formulae:
               	\begin{block}{Max Norm}
		       \textit{max\_norm = $\frac{old\_score}{max\_score}$} 
		\end{block}
                \begin{alertblock}{Min Max Norm}
		       \textit{min\_max\_norm = $\frac{old\_score - min\_score}{max\_score - min\_score}$}    
		\end{alertblock}
               
	\end{frame}

        \section{What about voting systems?}

        \begin{frame}{What about voting systems?}
               \textbf{Metaphor}: \textit{voters} are \textit{retrieval systems} \newline
                \hspace*{18mm}  \textit{candidates} are \textit{documents} \newline   
               The algorithm inspired by the \textbf{Condorcet method} starts from a
               document comparator known as \textit{Simple Majority Runoff}. \newline
               
	\end{frame}

        \section{``Simple Majority Runoff''}

        \begin{frame}{``Simple Majority Runoff''}
          Imagine a \textbf{contest} between two documents, then the rule below sounds sensible, doesn't it?: \newline \newline
          \texttt{The number of a document's votes is directly related to the number of times it ranks above the other.}
	\end{frame}

\end{document}
